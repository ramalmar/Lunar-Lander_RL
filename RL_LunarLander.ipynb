{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e3cc5e",
   "metadata": {},
   "source": [
    "# Solving the lunar lander environment using Reinforcement learning \n",
    "\n",
    "\n",
    "The purpose of this work is to develop a reinforcement learning model capable of solving the problem presented by the \"Moon Lander\" environment in the Gymnasium library. The objective is to train a model that can make decisions to successfully land the spaceship on the moon's surface with a smooth landing. Finally the evaluation of the working and performance wil be presented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71394c0d",
   "metadata": {},
   "source": [
    "## Environment analysis\n",
    "\n",
    "The enviroment used in this work is a optimization problem for the landind trajectory of a spaceship. The objective is to control the spaceship's landing by dealing with the controls and landing it in a specifical area of the moon. The goal is to achieve a smooth landing while minimizing fuel consumption.\n",
    "\n",
    "![Lunar Lander](https://gymnasium.farama.org/_images/lunar_lander.gif \"Episodio del lunar lander sin entrenar\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Observation\n",
    "\n",
    "The observation consists of 6 continuous and discrete values, which refer to the following aspects of the environment state:\n",
    "* Spaceship's x position\n",
    "* Spaceship's y position\n",
    "* Velocity in the x direction\n",
    "* Velocity in the y direction\n",
    "* Spaceship's angle\n",
    "* Angular velocity\n",
    "* Flag 1: 1 if the left leg of the spaceship is in contact with the moon's surface, 0 otherwise\n",
    "* Flag 2: 1 if the right leg of the spaceship is in contact with the moon's surface, 0 otherwise.\n",
    "\n",
    "\n",
    "\n",
    "### Actions\n",
    "\n",
    "There are 4 possible discrete actions (0 or 1) to be taken in each frame of the game:\n",
    "\n",
    "   * Do nothing\n",
    "   * Activate left orientation thruster\n",
    "   * Activate main bottom thruster\n",
    "   * Activate right orientation thruster\n",
    "\n",
    "The possible actions are selected through a list of 4 elements: [NOTHING, LEFT, MAIN, RIGHT].\n",
    "Rewards\n",
    "\n",
    "The environment provides a reward after each frame, depending on the actions taken. This reward can be positive or negative and is granted based on the following factors:\n",
    "\n",
    "   * It increases/decreases as the spaceship approaches/moves away from the landing platform.\n",
    "   * It increases/decreases as the spaceship moves slower/faster.\n",
    "   * It decreases as the spaceship becomes more tilted (non-horizontal angle).\n",
    "   * It increases by 10 points for each leg in contact with the ground.\n",
    "   * It decreases by 0.03 points for each frame in which the lateral thruster is activated.\n",
    "   * It decreases by 0.3 points for each frame in which the main thruster is activated.\n",
    "   * A final score of -100 points is received if the spaceship crashes or +100 points if the spaceship lands safely.\n",
    "\n",
    "After each frame, a reward is received based on the action taken. To obtain the reward for each episode, simply sum up the rewards obtained from all the frames that compose the episode.\n",
    "Expected Average Final Reward\n",
    "\n",
    "It will be considered successful completion of an episode if the sum of scores for all episodes results in +200 or above. This threshold is indicated in the environment documentation. However, the algorithm is set to stop training once this score is surpassed by calculating the average of the last trained episodes. The number of episodes used for averaging is determined by the parameter NUMBER_OF_REWARDS_TO_AVERAGE, which in this case is set to 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba21c5e4",
   "metadata": {},
   "source": [
    "## Proposed Solution\n",
    "\n",
    "To solve the problem at hand, we will use the Deep Q-learning (DQL) technique, which combines the Q-learning algorithm with deep neural networks. In essence, this method employs a neural network to approximate the Q-function, which evaluates the quality of actions in a given state. DQL has been chosen due to its ability to handle all possible states of the spaceship in an environment with 8 variables. This approach enables us to leverage the complexity of the observations and make optimal decisions at each moment.\n",
    "\n",
    "While the actor-critic technique could also have been utilized, DQL has been selected since the episodes in this environment are relatively short, and the problem is not as complex in comparison. This decision has been made with considerations for simplicity in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a940a4",
   "metadata": {},
   "source": [
    "##  Implementación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c4bb0",
   "metadata": {},
   "source": [
    "#### Preparatory Steps\n",
    "\n",
    "The main required libraries are imported, and the GPU device is set up to accelerate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2360b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d95b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Sí, hay una GPU disponible!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"¡Sí, hay una GPU disponible!\")\n",
    "else:\n",
    "    print(\"Lo siento, no se detectó una GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847bf67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb8765",
   "metadata": {},
   "source": [
    "#### Importing the Environment\n",
    "\n",
    "We import the Lunar Lander environment and set the render_mode to enable video visualization later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b51ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME=\"LunarLander-v2\"\n",
    "env=gym.make(ENV_NAME,render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e92c2",
   "metadata": {},
   "source": [
    "### Experience Buffer\n",
    "\n",
    "The following code section corresponds to the experience buffer, which plays a crucial role in training the model. Its main objective is to store a wide range of actions and the corresponding rewards, and then randomly sample these experiences. This prevents experiences from being returned with high correlation, which could negatively impact the training process by introducing biases due to such correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f159360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque  # Import the collections library to create deque data structures with a fixed size.\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))  # Initialize a deque structure with 4 possible variables.\n",
    "\n",
    "class ReplayMemory(object):  # Create the class that belongs to the experience buffer.\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)  # Initialize the structure.\n",
    "\n",
    "    def push(self, state, action, next_state, reward):  # Method to store a new experience in the structure.\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "\n",
    "    def sample(self, batch_size):  # Method to return random samples from the deque structure, the number of returned samples depends on the batch size.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):  # Method that returns the state of the memory.\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe57b9",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "\n",
    "In this class, a Deep Q-Network (DQN) model is implemented using PyTorch. The neural network takes the environment observations as input, and the number of outputs corresponds to the number of possible actions that the agent can take.\n",
    "\n",
    "With this model, the algorithm will be able to approximate the Q-value for each action in the current state. During training, the model takes the current environment observation as input and produces a prediction of the Q-function for each possible action as output.\n",
    "\n",
    "Once the training is completed, this model will be used to choose the best action given a specific state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ea3268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           1,152\n",
      "            Linear-2                  [-1, 128]          16,512\n",
      "            Linear-3                    [-1, 4]             516\n",
      "================================================================\n",
      "Total params: 18,180\n",
      "Trainable params: 18,180\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary # Importo la libreria torchsummary\n",
    "\n",
    "n_actions = env.action_space.n  # Get the number of actions in the environment\n",
    "state, info = env.reset()  # Get the initial state\n",
    "n_observations = len(state)  # Get the number of observations\n",
    "\n",
    "class DQN(nn.Module):  # Create the class for the model\n",
    "\n",
    "  def __init__(self,n_observations,n_actions):\n",
    "    super(DQN,self).__init__()\n",
    "    self.layer1 = nn.Linear(n_observations,128)\n",
    "    self.layer2 = nn.Linear(128,128)\n",
    "    self.layer3 = nn.Linear(128,n_actions)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x=F.relu(self.layer1(x))  \n",
    "    x=F.relu(self.layer2(x)) \n",
    "    return self.layer3(x) \n",
    "\n",
    "\n",
    "model_summary = DQN(n_observations, n_actions).to(device)  # Create the model\n",
    "summary(model_summary, (n_observations,))  # Print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b0be8",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "In the following section, the hyperparameters have been defined, which allow us to configure and adjust important variables of the training process to improve its performance and accuracy. Some of these parameters have been determined through trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe42991",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 # Number of experiences used to feed the neural network\n",
    "GAMMA = 0.99 # Discount factor\n",
    "LR = 1e-4 # Learning rate of the neural network\n",
    "\n",
    "EPS_START = 0.99 # Initial value of epsilon\n",
    "EPS_END = 0.02 # Final value of epsilon\n",
    "EPS_DECAY = 0.999985 # Epsilon decay rate per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff85e96",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The following class belongs to the agent, which is responsible for deciding which action to take based on a given state.\n",
    "\n",
    "### Agent's Policy\n",
    "\n",
    "This agent uses an e-greedy policy to choose an action at each step. This means it uses the epsilon probability to select a random action or use the trained neural network to obtain the Q-values of the possible actions and choose the best one using torch.max.\n",
    "\n",
    "Once we obtain the action, we apply it to the environment and receive the reward, the next state, or determine if the episode has ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85bee5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):# Initialize the environment within the class\n",
    "        self.env = env\n",
    "               \n",
    "    def step(self, net, state, epsilon=0.0, device=\"cpu\"):       \n",
    "         if np.random.random() > epsilon: # Exploration/Exploitation trade-off                            \n",
    "            with torch.no_grad():\n",
    "                return net(state).max(1)[1].view(1,1)  # Exploitation. Return the action from the model that yields the best Q-value\n",
    "         else:    \n",
    "                return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)  # Exploration. Return a random action to explore the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95767b25",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "\n",
    "The optimize_model function is responsible for using the Bellman equation to calculate the expected values of the actions. It uses the SmoothL1Loss loss function to optimize the neural network through gradient descent. Finally, the Adam optimization algorithm is used to update the weights of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8242f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    # Take a number of samples from the experience memory\n",
    "    transition = buffer.sample(BATCH_SIZE)  \n",
    "    \n",
    "    # Unpack the selected transitions from the previous line\n",
    "    batch = Transition(*zip(*transition))   \n",
    "    \n",
    "    # Get non-final states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]) \n",
    "            \n",
    "    # Create batches of states, actions, and rewards\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "    # Calculate Q-values of the actions taken in the current states\n",
    "    state_action_values = main_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Create a tensor of zeros of size BATCH_SIZE for the Q-values of the actions in the next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # If the transition is not final, calculate the Q-values of the actions in the next states\n",
    "    with torch.no_grad(): \n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "          \n",
    "    # Calculate the expected values using the Bellman equation\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch \n",
    "\n",
    "    # Compute the loss function using the Huber loss function\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(main_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb593ed",
   "metadata": {},
   "source": [
    "### Main Training Loop\n",
    "\n",
    "The following block will execute the main program that calls the previously created functions and classes in order to train the agent.\n",
    "Process Details\n",
    "\n",
    "Firstly, the main neural network model and the target are initialized. This allows the target model to not be updated immediately at each step and to be smoothly updated with the weights of the main model. Then, we will initialize the classes corresponding to the agent and the experience buffer, necessary for sampling experiences and selecting the best action following a specific policy.\n",
    "\n",
    "Once all the variables are declared, we enter a loop from which we cannot exit until the training is completed. This loop is divided into two phases: sampling and learning.\n",
    "\n",
    "In the sampling phase, the first thing we do is calculate a new value of epsilon, which will decrease at each iteration of the loop. This epsilon value is sent to the agent along with the current state. The agent will use an epsilon-greedy policy to return an action to perform, which will be stored in the experience buffer. Once the experience buffer reaches a certain number of stored experiences (in this case, 20000), the loop will also execute the learning phase, which is performed within the optimize method.\n",
    "\n",
    "In the learning phase, the first thing we do is obtain a batch of experiences from the experience memory (Experience Replay), and for each of them, we calculate the Q-function values using the model and the expected values using the main neural network model. Then, we calculate the expected values using the Bellman equation, in order to compare these last two values and calculate the model's loss.\n",
    "\n",
    "Finally, we use the necessary PyTorch functions to perform backpropagation through the model and allow it to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20 | Rewards: -162.90846 | Epsilon: 0.9628183207348258\n",
      "Episode: 40 | Rewards: -127.20722 | Epsilon: 0.9364812743339184\n",
      "Episode: 60 | Rewards: -132.57149 | Epsilon: 0.9112883083538216\n",
      "Episode: 80 | Rewards: -121.93794 | Epsilon: 0.8866400697480405\n",
      "Episode: 100 | Rewards: -111.68168 | Epsilon: 0.8612362853729478\n",
      "Episode: 120 | Rewards: -120.81178 | Epsilon: 0.8321050043658573\n",
      "Episode: 140 | Rewards: -124.89493 | Epsilon: 0.805588766577632\n",
      "Episode: 160 | Rewards: -89.93529 | Epsilon: 0.7816391357367317\n",
      "Episode: 180 | Rewards: -93.22845 | Epsilon: 0.7567310663476117\n",
      "Episode: 200 | Rewards: -78.92176 | Epsilon: 0.7344762822064665\n",
      "Episode: 220 | Rewards: -87.27168 | Epsilon: 0.7118608589234989\n",
      "Episode: 240 | Rewards: -50.63161 | Epsilon: 0.6908220356138054\n",
      "Episode: 260 | Rewards: -54.28645 | Epsilon: 0.6672447926193329\n",
      "Episode: 280 | Rewards: -50.44435 | Epsilon: 0.6458464225621191\n",
      "Episode: 300 | Rewards: -51.99422 | Epsilon: 0.6273041564829424\n",
      "Episode: 320 | Rewards: -41.49382 | Epsilon: 0.6088739681348684\n",
      "Episode: 340 | Rewards: -40.48043 | Epsilon: 0.5898251039831859\n",
      "Episode: 360 | Rewards: -41.77706 | Epsilon: 0.5730802980019253\n",
      "Episode: 380 | Rewards: -49.60712 | Epsilon: 0.5566021012352832\n",
      "Episode: 400 | Rewards: -42.85969 | Epsilon: 0.5398764922720694\n",
      "Episode: 420 | Rewards: -54.91926 | Epsilon: 0.5215838964972952\n",
      "Episode: 440 | Rewards: -37.00265 | Epsilon: 0.4988721262807082\n",
      "Episode: 460 | Rewards: -30.41160 | Epsilon: 0.47536334141698444\n",
      "Episode: 480 | Rewards: -37.27604 | Epsilon: 0.44708958169871854\n",
      "Episode: 500 | Rewards: -19.29743 | Epsilon: 0.418414883224063\n",
      "Episode: 520 | Rewards: -7.13003 | Epsilon: 0.3591669559463226\n",
      "Episode: 540 | Rewards: 8.06405 | Epsilon: 0.2960226254071242\n",
      "Episode: 560 | Rewards: 36.28479 | Epsilon: 0.23392700514870773\n",
      "Episode: 580 | Rewards: 111.70042 | Epsilon: 0.18126267275072097\n",
      "Episode: 600 | Rewards: 188.18718 | Epsilon: 0.15746940013650002\n",
      "Episode: 620 | Rewards: 182.97951 | Epsilon: 0.13166750093182922\n"
     ]
    }
   ],
   "source": [
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of state observation\n",
    "state, info = env.reset()\n",
    "\n",
    "# Number of features in the state\n",
    "n_observations = len(state)\n",
    "\n",
    "main_net = DQN(n_observations, n_actions).to(device)  # Initialize the main model\n",
    "target_net = DQN(n_observations, n_actions).to(device)  # Initialize the target network Q\n",
    "\n",
    "target_net.load_state_dict(main_net.state_dict())\n",
    "\n",
    "# Optimizer - AdamW used to optimize the weights\n",
    "optimizer = optim.AdamW(main_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "buffer = ReplayMemory(20000)  # Initialize the experience memory\n",
    "\n",
    "agent = Agent(env)\n",
    "\n",
    "epsilon = EPS_START\n",
    "\n",
    "reward_hist = []\n",
    "epsilon_hist = []\n",
    "mean_hist = []\n",
    "last_mean = 0\n",
    "steps = 0\n",
    "episode = 0\n",
    "\n",
    "while True:  # Episode loop\n",
    "\n",
    "    state, info = env.reset()  # Reset the environment\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    acum_rewards = 0  # Clear the reward accumulator\n",
    "\n",
    "    while True:  # Frame loop\n",
    "\n",
    "        steps += 1  # Add one step or frame to the total number of steps\n",
    "        epsilon = max(epsilon * EPS_DECAY, EPS_END)  # Calculate the epsilon for this episode\n",
    "\n",
    "        action = agent.step(main_net, state, epsilon, device)  # Call the agent to take an action based on the current state and epsilon\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())  # Execute the action on the environment and get the new state, reward, and check if the episode has finished\n",
    "        acum_rewards += reward  # Accumulate the rewards per episode\n",
    "        reward = torch.tensor([reward], device=device)  # Convert the obtained reward to a PyTorch tensor\n",
    "\n",
    "        done = terminated or truncated  # Flag for episode termination\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)  # If the episode has not finished, observe the new state and convert it to a tensor\n",
    "\n",
    "        buffer.push(state, action, next_state, reward)  # Store the transition in the experience replay memory D\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "        if len(buffer) > BATCH_SIZE:  # We don't optimize until the experience memory has a minimum number of experiences\n",
    "            optimize_model()  # Call the function to optimize the model\n",
    "\n",
    "        # Synchronize the parameters of the main model with those of the target network every certain number of frames\n",
    "        if steps % 1000 == 0:\n",
    "            target_net.load_state_dict(main_net.state_dict())\n",
    "\n",
    "        # If the episode ends\n",
    "        if done:\n",
    "            reward_hist.append(acum_rewards)  # Save the accumulated rewards in a list\n",
    "            epsilon_hist.append(epsilon)  # Save the last epsilon\n",
    "            episode += 1  # Add one episode to the number of episodes\n",
    "            break\n",
    "\n",
    "    if episode >= 20:\n",
    "        mean_hist.append(np.mean(reward_hist[-20:]))\n",
    "        last_mean = mean_hist[-1:][0]\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        print(f\"Episode: {episode} | Rewards: {last_mean:.5f} | Epsilon: {epsilon}\")\n",
    "\n",
    "    if last_mean == 200:\n",
    "        print(\"Fin\")\n",
    "        torch.save(main_net.state_dict(), ENV_NAME + \".dat\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2850d",
   "metadata": {},
   "source": [
    "### Training Model Results\n",
    "\n",
    "Once the average of the rewards from the last 20 episodes exceeded 250 points, it is considered that the training has finished. This score is the threshold set to conclude the training. We observe a positive progression in the rewards throughout the process, as they evolved from initial negative values to significantly higher scores. This evolution is a clear indication that the agent has learned to take the correct actions in order to maximize its final score.\n",
    "\n",
    "It is important to note that the training of the agent was not linear. Initially, the rewards were low and negative, reflecting the initial learning period where the agent explored the environment and tried different actions. As the training progressed, the rewards started to improve progressively, indicating that the agent was acquiring knowledge and optimizing its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd080f",
   "metadata": {},
   "source": [
    "# Graphs of Results\n",
    "\n",
    "The following code will allow us to visualize the graphs that demonstrate the agent's evolution. One of them shows the average of the rewards from the current episode and the previous 19 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, sharex=True, figsize=(12, 20))\n",
    "\n",
    "# Plot the first graph with mean_hist values\n",
    "ax1.plot(mean_hist)\n",
    "ax1.set_ylabel('Mean Reward')\n",
    "ax1.set_title('Mean Rewards of Last 20 Episodes')\n",
    "\n",
    "# Plot the epsilon_hist values in the second graph\n",
    "ax3.plot(epsilon_hist)\n",
    "ax3.set_xlabel('Episodes')\n",
    "ax3.set_title('Epsilon Value per Episode')\n",
    "ax3.set_ylabel('Epsilon')\n",
    "\n",
    "# Plot the reward_hist values in the third graph\n",
    "ax2.plot(reward_hist)\n",
    "ax2.set_title('Instantaneous Reward History')\n",
    "ax2.set_ylabel('Reward')\n",
    "\n",
    "# Adjust the Y-axis limits\n",
    "ax1.set_ylim([min(mean_hist), max(mean_hist)])\n",
    "ax3.set_ylim([0, 1.0])\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "\n",
    "# Show the resulting figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7bb47e",
   "metadata": {},
   "source": [
    "The graphs clearly show how the model improves the rewards obtained per episode as it learns the best actions to take in each state. The learning process ends when an average reward of 250 or higher is achieved. In the last graph, we can observe how the value of \"epsilon\" decreases as the training progresses. This implies a reduction in the exploration of random actions and an increase in the exploitation of actions that have shown to generate the best rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495fe72",
   "metadata": {},
   "source": [
    "#### Visualization of an Episode with Trained Model\n",
    "\n",
    "As a final step, the Lunar Lander environment has been imported again, and the weights of the previously trained model have been loaded. A game was played, and the entire episode was recorded as a video using the RecordVideo wrapper, which captures and stores the complete sequence. At the end, it can be observed how the algorithm is able to guide the spacecraft for a successful landing on the moon. The resulting video showcases the application of the learned actions during training and demonstrates the model's successful performance in solving the Lunar Lander challenge.\n",
    "\n",
    "A pre-generated video is available in the Video folder within the delivered working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26216893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "#Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#Get the number of state observaation\n",
    "state,info = env.reset()\n",
    "\n",
    "#number of features in the state\n",
    "n_observations = len(state)\n",
    "\n",
    "weights = torch.load(\"LunarLander-v2\" + \".dat\")\n",
    "\n",
    "model= DQN(n_observations,n_actions).to(device) \n",
    "\n",
    "model.load_state_dict(weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "env_test = gym.make('LunarLander-v2',                  \n",
    "                    render_mode= \"rgb_array\"\n",
    "                   )\n",
    "env_test  = gym.wrappers.RecordVideo(env_test , 'video')\n",
    "reward=0\n",
    "\n",
    "observation ,_  = env_test.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    # Convierte la observación a un tensor y realiza una inferencia con el modelo\n",
    "    observation_tensor = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "    action_probs = model(observation_tensor)\n",
    "    \n",
    "    # Elige la acción con mayor probabilidad\n",
    "    action = torch.argmax(action_probs).item()\n",
    "\n",
    "    # Realiza la acción y recibe la siguiente observación, la recompensa y si el juego ha terminado\n",
    "    observation, reward, done, info,_ = env_test.step(action)\n",
    "    reward+=reward\n",
    "    if reward==200:\n",
    "        break\n",
    "    \n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330864e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"video/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6411505",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "### Analysis of Results\n",
    "\n",
    "As we have observed, the training results have been satisfactory, as the agent has learned to make the correct control decisions to perform a quick and safe landing of the spacecraft. Once trained, the agent is able to achieve a reward of 200 in each episode, which is sufficient to consider the game successfully completed according to the environment's documentation.\n",
    "### Other Tested Environments\n",
    "\n",
    "In addition to this environment, an attempt was made to train a model for the ATARI game \"Skiing,\" but the expected results were not obtained. A similar approach to this work was used, but with a convolutional neural network model to take game frames as observations. However, the training process took a long time due to the duration of an episode, which ended when the skier crossed the finish line. For this reason, a simpler environment was chosen that could be trained with my GPU in a reasonable time.\n",
    "### Lessons Learned\n",
    "\n",
    "Throughout the development of this project, valuable knowledge has been acquired in various key areas. Firstly, skills in designing neural network architectures have been developed, enabling the creation of an effective model to address a specific reinforcement learning problem. Additionally, experience has been gained in working with simulation environments provided by the Gym library.\n",
    "\n",
    "One notable advantage of Gym's environments is their accessibility, allowing anyone interested in learning about reinforcement learning to quickly start developing models tailored to the specific needs of each environment. This ease of use has enabled the application of acquired theoretical knowledge and exploration of different approaches and techniques in a controlled environment.\n",
    "\n",
    "Furthermore, through this project, a deeper understanding of theoretical and practical concepts related to reinforcement learning has been achieved. First-hand experience has been gained in applying these concepts to real-world problems and the importance of experimentation to achieve optimal results.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Upon completing this project, it would be a good step to attempt training a model for a more complex environment than the one used in this work. A natural progression would be to devote more efforts to modeling an agent capable of dealing with an ATARI environment. It would also be beneficial to explore other types of reinforcement learning techniques to further expand knowledge in this fascinating field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04562367",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "\n",
    "    * Torres, J. (2021). Introducción al aprendizaje por refuerzo profundo. Teoría y práctica en Python. Amazon.\n",
    "    * PyTorch Official Website. Retrieved from: https://pytorch.org/docs/stable/index.html\n",
    "    * Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG). Retrieved from: https://medium.com/towards-data-science/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc0bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
